{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 02 - Deep Learning Repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Even though the metrics looked quite good in the basic model, the performance of classifying custom hand-written digits was sub-optimal.\n",
    "\n",
    "In this notebook, we address this issue by performing **data augmentation** on the dataset to account for variations such as rotations, different pencil shapes, blurs, and more.\n",
    "\n",
    "These modifications should improve the model's ability to recognize custom hand-written digits. ðŸ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "import sklearn.metrics\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import onnxruntime\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "There are a few options to augment the image data:\n",
    "\n",
    "- Randomly crop the image: RandomCrop(Output Image Size, Padding of the Image)\n",
    "\n",
    "    `torchvision.transforms.RandomCrop(28, 4)`\n",
    "\n",
    "- Randomly rotate the image: RandomRotation(Max Rotation in Degrees)\n",
    "\n",
    "    `torchvision.transforms.RandomRotation(5)`\n",
    "\n",
    "- Randomly blur the image: GaussianBlur(Kernel Size) -> larger kernel means more blur\n",
    "\n",
    "    `torchvision.transforms.GaussianBlur(kernel_size = (5,5))`\n",
    "\n",
    "- Randomly change the colors in the image: ColorJitter(Brightness, Contrast, Saturation, Hue)\n",
    "\n",
    "    `torchvision.transforms.ColorJitter(brightness=(0.5,1.5), contrast=3, saturation=(0.3,1.5), hue=(-0.1,0.1))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation\n",
    "image_transformation = transforms.Compose(\n",
    "    [\n",
    "        # '''TODO: Insert data augmentation here.'''\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_directory = Path.cwd() / \".mnist_data\" / \"2_augmented\"\n",
    "\n",
    "# Delete the existing MNIST data directory if it exists\n",
    "if mnist_directory.exists() and mnist_directory.is_dir():\n",
    "    shutil.rmtree(mnist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "dataset_mnist_train = datasets.MNIST(\n",
    "    mnist_directory, train=True, download=True, transform=image_transformation\n",
    ")\n",
    "dataset_mnist_test = datasets.MNIST(\n",
    "    mnist_directory, train=False, download=True, transform=image_transformation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training dataset into training and validation sets using PyTorch's `random_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset_mnist_train))\n",
    "val_size = len(dataset_mnist_train) - train_size\n",
    "dataset_mnist_train, dataset_mnist_val = random_split(\n",
    "    dataset_mnist_train, [train_size, val_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataLoader` class is useful to create batch sizes of the data and to shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(dataset_mnist_train, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset_mnist_val, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset_mnist_test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Dataset\n",
    "\n",
    "Let us briefly look at the data ðŸ‘€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(f\"Image Batch shape: {images.shape}\")\n",
    "print(f\"Label Batch shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3)\n",
    "fig.suptitle(\"MNIST Dataset Samples\", fontsize=16)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    idx = torch.randint(0, len(images), (1,))\n",
    "    ax.imshow(images[idx].squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {labels[idx].item()}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We still use the same simple CNN to better compare the results (change one thing at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=4 * 14 * 14, out_features=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary(model=model, input_size=images.shape))\n",
    "model_output = model(images)\n",
    "print(f\"Model output shape: {model_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_INTERVAL = 100  # Calculate losses after every 100 batches\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for batch_idx, (image_batch, output_batch) in enumerate(\n",
    "        tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\"), start=1\n",
    "    ):\n",
    "        image_batch, output_batch = image_batch.to(device), output_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model_output = model(image_batch)\n",
    "        loss = loss_function(model_output, output_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "        if (batch_idx) % BATCH_INTERVAL == 0:\n",
    "            avg_train_loss = running_train_loss / (batch_idx)\n",
    "            train_loss_history.append(avg_train_loss)\n",
    "            print(f\"After batch {batch_idx}: Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Evaluate validation loss\n",
    "            model.eval()\n",
    "            running_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_image_batch, val_output_batch in tqdm(\n",
    "                    val_loader, desc=\"Validation\"\n",
    "                ):\n",
    "                    val_image_batch, val_output_batch = (\n",
    "                        val_image_batch.to(device),\n",
    "                        val_output_batch.to(device),\n",
    "                    )\n",
    "                    val_model_output = model(val_image_batch)\n",
    "                    val_loss = loss_function(val_model_output, val_output_batch)\n",
    "                    running_val_loss += val_loss.item()\n",
    "            avg_val_loss = running_val_loss / len(val_loader)\n",
    "            val_loss_history.append(avg_val_loss)\n",
    "            print(f\"After batch {batch_idx}: Validation Loss: {avg_val_loss:.4f}\")\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "    avg_epoch_train_loss = running_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluate validation loss at the end of each epoch\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_image_batch, val_output_batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            val_model_output = model(val_image_batch)\n",
    "            val_loss = loss_function(val_model_output, val_output_batch)\n",
    "            running_val_loss += val_loss.item()\n",
    "    avg_epoch_val_loss = running_val_loss / len(val_loader)\n",
    "    val_loss_history.append(avg_epoch_val_loss)\n",
    "    print(f\"Epoch {epoch+1} - Validation Loss: {avg_epoch_val_loss:.4f}\")\n",
    "\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "Test the freshly trained model with a randomly drawn image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_onnx_model(path_to_model: Path):\n",
    "    if path_to_model is None:\n",
    "        raise ValueError(\"Path must be provided if onnx is True\")\n",
    "    return onnxruntime.InferenceSession(\n",
    "        str(path_to_model), providers=[\"CPUExecutionProvider\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def predict_digit(\n",
    "    input_tensor: torch.Tensor,\n",
    "    model: torch.nn.Module = None,\n",
    "    onnx: bool = False,\n",
    "    onnx_model_path: Path = None,\n",
    ") -> tuple[int, float]:\n",
    "    if onnx:\n",
    "        model_session = load_onnx_model(onnx_model_path)\n",
    "        onnx_predictions = model_session.run(None, {\"image\": input_tensor})\n",
    "        predictions = torch.tensor(np.array(onnx_predictions))\n",
    "    else:\n",
    "        predictions = model(input_tensor)\n",
    "\n",
    "    predicted_number = torch.argmax(predictions)\n",
    "    probabilities = F.softmax(predictions.clone().detach(), dim=-1).numpy().squeeze()\n",
    "    confidence = probabilities[predicted_number]\n",
    "    return predicted_number.item(), confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_idx = torch.randint(0, len(images), (1,))\n",
    "test_img = images[sampling_idx]\n",
    "test_label = labels[sampling_idx]\n",
    "plt.imshow(test_img.numpy().squeeze(), cmap=\"grey\")\n",
    "plt.show()\n",
    "predicted_number, confidence = predict_digit(test_img, model)\n",
    "print(f\"Predicted : {predicted_number} - Confidence: {100*confidence:.2f}%\")\n",
    "print(f\"True label: {test_label.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Does it look good?\n",
    "\n",
    "Since this is just one image from the **training** dataset, we need to evaluate how the model performs on data it has never seen before. This is where the **test** dataset comes in. In the following cell, we calculate various metrics to evaluate the model's performance.\n",
    "\n",
    "The metrics we will calculate are Accuracy, Recall, F1 score, and Support:\n",
    "\n",
    "- **Accuracy**: The ratio of correctly predicted instances to the total instances. It gives an overall measure of how well the model is performing.\n",
    "- **Recall**: The ratio of correctly predicted positive observations to all the observations in the actual class. It tells us how well the model can identify positive instances.\n",
    "- **F1 Score**: The weighted average of Precision and Recall. It is useful when you need a balance between Precision and Recall.\n",
    "- **Support**: The number of actual occurrences of the class in the dataset. It helps in understanding the distribution of the dataset.\n",
    "\n",
    "When interpreting these metrics, keep in mind:\n",
    "- **Accuracy** can be misleading if the dataset is imbalanced.\n",
    "- **Recall** is crucial when the cost of false negatives is high.\n",
    "- **F1 Score** is useful when you need to balance Precision and Recall.\n",
    "- **Support** helps in understanding the context of the other metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "test_predicted_output = []\n",
    "test_true_output = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (image_batch, output_batch) in enumerate(\n",
    "        tqdm(test_loader, desc=\"Evaluating the model on the Test set\")\n",
    "    ):\n",
    "        image_batch, output_batch = image_batch.to(device), output_batch.to(device)\n",
    "        model_output = model(image_batch)\n",
    "        predicted_numbers = torch.argmax(model_output, dim=1)\n",
    "        test_predicted_output.extend(predicted_numbers.cpu().numpy())\n",
    "        test_true_output.extend(output_batch.cpu().numpy())\n",
    "\n",
    "cm_abs = sklearn.metrics.confusion_matrix(\n",
    "    test_true_output, test_predicted_output, labels=[i for i in range(10)]\n",
    ")\n",
    "cm_norm = sklearn.metrics.confusion_matrix(\n",
    "    test_true_output,\n",
    "    test_predicted_output,\n",
    "    labels=[i for i in range(10)],\n",
    "    normalize=\"true\",\n",
    ")\n",
    "cm_round = np.around(cm_norm, 2)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "disp_abs = sklearn.metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_abs, display_labels=[i for i in range(10)]\n",
    ")\n",
    "disp_norm = sklearn.metrics.ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_round, display_labels=[i for i in range(10)]\n",
    ")\n",
    "\n",
    "classification_report_test = sklearn.metrics.classification_report(\n",
    "    test_true_output, test_predicted_output, labels=[i for i in range(10)]\n",
    ")\n",
    "\n",
    "disp_abs.plot(ax=ax1)\n",
    "ax1.set_title(\"Absolute\")\n",
    "ax1.grid(False)\n",
    "\n",
    "disp_norm.plot(ax=ax2)\n",
    "ax2.set_title(\"Normalized\")\n",
    "ax2.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(classification_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a simple CNN and augmented data, the metrics tend to be lower than in the other notebooks. You can add another epoch, which might improve the scores. Nonetheless, let us see how well the model performs on your hand-written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"./2_augmented_mnist_model.onnx\"\n",
    "\n",
    "MODEL_FOLDER = Path.cwd() / \"models\"\n",
    "MODEL_PATH = MODEL_FOLDER / FILE_NAME\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not MODEL_FOLDER.exists():\n",
    "    MODEL_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Directory '{MODEL_FOLDER}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{MODEL_FOLDER}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.eval()\n",
    "image = images[0].unsqueeze(0)\n",
    "torch.onnx.export(\n",
    "    model, image, MODEL_PATH, input_names=[\"image\"], output_names=[\"digit\"]\n",
    ")\n",
    "\n",
    "print(f\"Model exported in ONNX format to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world Data\n",
    "\n",
    "Most tutorials would just finish with the above evaluation. After all, the metrics look quite good, the majority of the manually provided samples to the model are correctly classified; what more do you want?\n",
    "\n",
    "What is mostly missing, but what is the most common problem in practice, is, that there are real-world cases with real-world data. The model is performing quite well, but what if you provide completely novel hand-written digits to it that is maybe not nicely centred, or is drawn with a different pencil width, or rotated, etc.\n",
    "\n",
    "In the following section, we add this real-world scenario by letting you draw your own hand-written digits and test your trained models against this custom dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Digit\n",
    "\n",
    "To create a single digit that can be run in the `Real-World Model Inference` below:\n",
    "1. Open the drawing tool to create hand-written digits yourself by running the next cell\n",
    "1. Select the model you just created\n",
    "1. Draw a digit\n",
    "1. Click on 'Classify' to see how well the model predicts\n",
    "1. Close the tool (or try more digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Digits Test Dataset\n",
    "\n",
    "To create a suite of digits ranging from 0 to 9 that can be run in the `Real-World Digits Comparison` below:\n",
    "1. Open the drawing tool to create hand-written digits yourself by running the next cell\n",
    "1. Click on `Create Custom Test Set`\n",
    "1. Draw the digit that is prompted\n",
    "1. Click on `Save and Next`\n",
    "1. Once done you can change any drawn number by selecting it from the dropdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/model_test_drawing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    Path.cwd() / \"handwriting\" / \"my_digit.npy\"\n",
    ").exists(), \"Draw and classify a digit first.\"\n",
    "image_input_tensor = np.load(\"handwriting/my_digit.npy\")\n",
    "\n",
    "plt.cla()\n",
    "plt.imshow(image_input_tensor[0].squeeze(), cmap=\"grey\")\n",
    "plt.title(\"Input Tensor\")\n",
    "plt.show()\n",
    "\n",
    "try:\n",
    "    model.eval()\n",
    "    pt_predicted_number, pt_confidence = predict_digit(\n",
    "        torch.tensor(image_input_tensor), model\n",
    "    )\n",
    "    print(\n",
    "        f\"PyTorch model prediction: {pt_predicted_number} - Confidence: {100*pt_confidence:.2f}%\"\n",
    "    )\n",
    "except NameError:\n",
    "    print(\"No PyTorch model present, skipping...\")\n",
    "\n",
    "onnx_predicted_number, onnx_confidence = predict_digit(\n",
    "    image_input_tensor, onnx=True, onnx_model_path=MODEL_PATH\n",
    ")\n",
    "print(\n",
    "    f\"ONNX model prediction:    {onnx_predicted_number} - Confidence: {100*onnx_confidence:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-World Digits Comparison\n",
    "\n",
    "The plot lets you compare the performance of all the models from the following notebooks (they will have the same output, just with their model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    Path.cwd() / \"handwriting\" / \"digit_0.npy\"\n",
    ").exists(), \"Draw and classify a digit first.\"\n",
    "directory_path = Path(\"handwriting/\")\n",
    "\n",
    "fig, axes = plt.subplots(10, 2, figsize=(5, 20))\n",
    "\n",
    "for i in range(10):\n",
    "    file_path = directory_path / f\"digit_{i}.npy\"\n",
    "    image_input_tensor = np.load(file_path)\n",
    "\n",
    "    axes[i, 0].imshow(image_input_tensor.squeeze(), cmap=\"gray\")\n",
    "    axes[i, 0].set_title(f\"Digit {i}\")\n",
    "    axes[i, 0].axis(\"off\")\n",
    "\n",
    "    onnx_predicted_number, onnx_confidence = predict_digit(\n",
    "        image_input_tensor, onnx=True, onnx_model_path=MODEL_PATH\n",
    "    )\n",
    "\n",
    "    axes[i, 1].text(\n",
    "        0.5, 0.6, f\"Model prediction: {onnx_predicted_number}\", fontsize=12, ha=\"center\"\n",
    "    )\n",
    "    axes[i, 1].text(\n",
    "        0.5, 0.4, f\"Confidence: {100*onnx_confidence:.2f}%\", fontsize=12, ha=\"center\"\n",
    "    )\n",
    "    axes[i, 1].axis(\"off\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that even though we augmented the dataset with one or more transformations, the model is still not able to classify well on your drawings.\n",
    "\n",
    "The model may still be wrong, but hopefully with a lower confidence than the basic model.\n",
    "\n",
    "In the final notebook, we will address this issue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
